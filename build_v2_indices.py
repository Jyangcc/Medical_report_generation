import numpy as np
import pickle
import faiss
import json
import os
from typing import List, Dict, Any

FEATURES_V2_FILE = 'mammography_features_v2.pkl'
TRAIN_CASES_FILE = 'train_cases.json'

def load_features_v2(features_file='mammography_features_v2.pkl'):
    print(f"ğŸ”„ è¼‰å…¥ V2 ç‰¹å¾µæª”æ¡ˆ: {features_file}")
    with open(features_file, 'rb') as f:
        features_dict = pickle.load(f)
    print(f"âœ… è¼‰å…¥å®Œæˆï¼Œå…± {len(features_dict)} å€‹æ¡ˆä¾‹")
    return features_dict

def load_train_ids(train_file=TRAIN_CASES_FILE):
    if not os.path.exists(train_file):
        print(f"âŒ éŒ¯èª¤: æ‰¾ä¸åˆ°è¨“ç·´é›†æª”æ¡ˆ {train_file}")
        print("è«‹å…ˆåŸ·è¡Œ 'prepare_split.py'")
        return None
    
    print(f"ğŸ”„ è¼‰å…¥è¨“ç·´é›† ID: {train_file}")
    with open(train_file, 'r') as f:
        train_ids = json.load(f)
    print(f"âœ… æˆåŠŸè¼‰å…¥ {len(train_ids)} å€‹è¨“ç·´é›† ID")
    return set(train_ids) # ä½¿ç”¨ set åŠ é€ŸæŸ¥æ‰¾

def build_global_index(features_dict: Dict[str, Any], train_ids: set, feature_type: str):
    """
    [ä¿®æ”¹ç‰ˆ]
    åªä½¿ç”¨ 'train_ids' ä¸­çš„æ¡ˆä¾‹ä¾†å»ºç«‹ã€Œå…¨åŸŸç‰¹å¾µã€ç´¢å¼•
    """
    print(f"\nğŸ”¨ å»ºç«‹ã€Œå…¨åŸŸã€ç´¢å¼• (åƒ…è¨“ç·´é›†): {feature_type}")
    
    features_list = []
    case_ids_map = [] 
    
    # ï¼ï¼ï¼é—œéµä¿®æ”¹ï¼ï¼ï¼
    # åªéæ­· features_dict ä¸­ï¼ŒID åœ¨ train_ids è£¡çš„æ¡ˆä¾‹
    for case_id in train_ids:
        if case_id not in features_dict:
            continue
            
        case_data = features_dict[case_id]
        global_features = case_data.get('features', {})
        
        if feature_type in global_features:
            features_list.append(global_features[feature_type])
            case_ids_map.append(case_id)

    if not features_list:
        print(f"âŒ æ²’æœ‰æ‰¾åˆ°ä»»ä½• {feature_type} ç‰¹å¾µï¼Œç„¡æ³•å»ºç«‹ç´¢å¼•")
        return

    features_array = np.array(features_list).astype('float32')
    feature_dim = features_array.shape[1]
    
    index = faiss.IndexFlatIP(feature_dim)
    index.add(features_array)
    
    print(f"âœ… {feature_type} ç´¢å¼•å»ºç«‹å®Œæˆ (å…± {index.ntotal} å€‹*è¨“ç·´*å‘é‡)")
    
    # å„²å­˜ç´¢å¼• (æˆ‘å€‘ä½¿ç”¨ç›¸åŒçš„æª”åï¼Œè¦†è“‹æ‰èˆŠçš„ "ä½œå¼Š" ç´¢å¼•)
    index_file = f"faiss_global_{feature_type}.index"
    metadata_file = f"faiss_global_{feature_type}_map.pkl"
    
    faiss.write_index(index, index_file)
    with open(metadata_file, 'wb') as f:
        pickle.dump(case_ids_map, f)
        
    print(f"ğŸ’¾ ç´¢å¼•å·²å„²å­˜: {index_file}, {metadata_file}")

def build_lesion_index(features_dict: Dict[str, Any], train_ids: set):
    """
    [ä¿®æ”¹ç‰ˆ]
    åªä½¿ç”¨ 'train_ids' ä¸­çš„æ¡ˆä¾‹ä¾†å»ºç«‹ã€Œç—…ç¶ç‰¹å¾µã€ç´¢å¼•
    """
    print(f"\nğŸ”¨ å»ºç«‹ã€Œç—…ç¶ (ROI)ã€ç´¢å¼• (åƒ…è¨“ç·´é›†)...")
    
    lesion_features = []
    lesion_metadata_map = [] 
    views_to_check = ['RCC', 'LCC', 'RMLO', 'LMLO']
    
    # ï¼ï¼ï¼é—œéµä¿®æ”¹ï¼ï¼ï¼
    for case_id in train_ids:
        if case_id not in features_dict:
            continue
            
        case_data = features_dict[case_id]
        case_features = case_data.get('features', {})
        for view in views_to_check:
            if view in case_features:
                view_data = case_features[view]
                for lesion in view_data.get('lesions', []):
                    lesion_features.append(lesion['roi_feature'])
                    lesion_metadata_map.append({
                        'case_id': case_id,
                        'view': view,
                        'bbox': lesion['bbox'],
                        'conf': lesion['conf']
                    })

    if not lesion_features:
        print("âš ï¸ è­¦å‘Š: è¨“ç·´é›†ä¸­æ²’æœ‰æ‰¾åˆ°ä»»ä½•ç—…ç¶ï¼Œç—…ç¶ç´¢å¼•å°‡æ˜¯ç©ºçš„")
        return

    features_array = np.array(lesion_features).astype('float32')
    feature_dim = features_array.shape[1]
    
    index = faiss.IndexFlatIP(feature_dim)
    index.add(features_array)
    
    print(f"âœ… ç—…ç¶ç´¢å¼•å»ºç«‹å®Œæˆ (å…± {index.ntotal} å€‹*è¨“ç·´*ç—…ç¶å‘é‡)")
    
    index_file = f"faiss_lesion_roi.index"
    metadata_file = f"faiss_lesion_roi_map.pkl"
    
    faiss.write_index(index, index_file)
    with open(metadata_file, 'wb') as f:
        pickle.dump(lesion_metadata_map, f)
        
    print(f"ğŸ’¾ ç´¢å¼•å·²å„²å­˜: {index_file}, {metadata_file}")

if __name__ == "__main__":
    
    try:
        import faiss
    except ImportError:
        print("âŒ éŒ¯èª¤: æ‰¾ä¸åˆ° 'faiss-cpu'ã€‚è«‹å®‰è£: pip install faiss-cpu")
        exit()

    # 1. è¼‰å…¥å®Œæ•´çš„ V2 ç‰¹å¾µ
    features_data = load_features_v2()
    
    # 2. è¼‰å…¥è¨“ç·´é›† ID
    train_case_ids = load_train_ids()
    
    if train_case_ids:
        # 3. å»ºç«‹æ‰€æœ‰ã€Œå…¨åŸŸã€ç´¢å¼• (åƒ…è¨“ç·´é›†)
        build_global_index(features_data, train_case_ids, 'avg_all_global')
        build_global_index(features_data, train_case_ids, 'avg_right_global')
        build_global_index(features_data, train_case_ids, 'avg_left_global')
        
        # 4. å»ºç«‹ã€Œç—…ç¶ã€ç´¢å¼• (åƒ…è¨“ç·´é›†)
        build_lesion_index(features_data, train_case_ids)
        
        print("\nğŸ‰ éšæ®µäºŒ (ä¿®æ­£ç‰ˆ) å®Œæˆï¼æ‰€æœ‰ç´¢å¼•åº«å‡å·²æ ¹æ“šè¨“ç·´é›†é‡å»ºã€‚")